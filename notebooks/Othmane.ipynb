{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "#from tqdm import tqdm\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods for Machine Learning\n",
    "\n",
    "## Data Loading and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train features\n",
    "df_Xtr0 = pd.read_csv(\"Data/Xtr0.csv\")\n",
    "df_Xtr1 = pd.read_csv(\"Data/Xtr1.csv\")\n",
    "df_Xtr2 = pd.read_csv(\"Data/Xtr2.csv\")\n",
    "\n",
    "df_Xtr0_mat100 = pd.read_csv(\"Data/Xtr0_mat100.csv\", header=None, sep=' ')\n",
    "df_Xtr1_mat100 = pd.read_csv(\"Data/Xtr1_mat100.csv\", header=None, sep=' ')\n",
    "df_Xtr2_mat100 = pd.read_csv(\"Data/Xtr2_mat100.csv\", header=None, sep=' ')\n",
    "\n",
    "# Train labels\n",
    "df_Ytr0 = pd.read_csv(\"Data/Ytr0.csv\")\n",
    "df_Ytr1 = pd.read_csv(\"Data/Ytr1.csv\")\n",
    "df_Ytr2 = pd.read_csv(\"Data/Ytr2.csv\")\n",
    "\n",
    "# Test features\n",
    "df_Xte0 = pd.read_csv(\"Data/Xte0.csv\")\n",
    "df_Xte1 = pd.read_csv(\"Data/Xte1.csv\")\n",
    "df_Xte2 = pd.read_csv(\"Data/Xte2.csv\")\n",
    "\n",
    "df_Xte0_mat100 = pd.read_csv(\"Data/Xte0_mat100.csv\", header=None, sep=' ')\n",
    "df_Xte1_mat100 = pd.read_csv(\"Data/Xte1_mat100.csv\", header=None, sep=' ')\n",
    "df_Xte2_mat100 = pd.read_csv(\"Data/Xte2_mat100.csv\", header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gaussian Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need to parallize its computation\n",
    "def GaussKernel(X1, X2, sigma = 1):\n",
    "    n, m = X1.shape[0], X2.shape[0]\n",
    "    K = np.zeros((n,m))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            K[i,j] = np.sqrt((1/(2*np.pi*sigma**2)))*np.exp(-((np.linalg.norm(X1[i]-X2[j]))**2)/(2*sigma**2))\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the Kernel matrix for each of the tree train sets and we save them in *Kernel_Matrix* directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranforming into numpy.arrays -- train\n",
    "Xtr0_mat100 = np.array(df_Xtr0_mat100)\n",
    "Xtr1_mat100 = np.array(df_Xtr1_mat100)\n",
    "Xtr2_mat100 = np.array(df_Xtr2_mat100)\n",
    "\n",
    "# Tranforming into numpy.arrays -- test\n",
    "Xte0_mat100 = np.array(df_Xte0_mat100)\n",
    "Xte1_mat100 = np.array(df_Xte1_mat100)\n",
    "Xte2_mat100 = np.array(df_Xte2_mat100)\n",
    "\n",
    "# Transforming the labels into numpy.arrays \n",
    "y0 = np.array(df_Ytr0)[:,1]\n",
    "y1 = np.array(df_Ytr1)[:,1]\n",
    "y2 = np.array(df_Ytr2)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should parallelize this computation\n",
    "K_Xtr0 = GaussKernel(Xtr0_mat100, Xtr0_mat100)\n",
    "np.save(\"Kernel_Matrix/gaussian_kernel_Xtr0.npy\",K_Xtr0)\n",
    "\n",
    "K_Xtr1 = GaussKernel(Xtr1_mat100, Xtr1_mat100)\n",
    "np.save(\"Kernel_Matrix/gaussian_kernel_Xtr1.npy\",K_Xtr1)\n",
    "\n",
    "K_Xtr2 = GaussKernel(Xtr2_mat100, Xtr2_mat100)\n",
    "np.save(\"Kernel_Matrix/gaussian_kernel_Xtr2.npy\",K_Xtr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Implement SVM with gaussian kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve the optimization problem $$\\left\\{\\begin{matrix}\n",
    "\\underset{\\alpha \\in \\mathbb{R}^{n}}{\\text{max}} \\hspace{0.1cm} 2\\alpha^{T}y - \\alpha^{T}K\\alpha \\\\ 0 \\leq y_i\\alpha_i \\leq \\frac{1}{2\\lambda n}, \\hspace{0.5cm} \\text{for} \\hspace{0.3cm} i = 0...n\n",
    "\\end{matrix}\\right. \\Leftrightarrow \\left\\{\\begin{matrix}\n",
    "\\underset{\\alpha \\in \\mathbb{R}^{n}}{\\text{min}} \\hspace{0.1cm} \\frac{1}{2}\\alpha^{T}P\\alpha + q^{t}\\alpha  \\\\ G\\alpha \\leq h\n",
    "\\end{matrix}\\right.   $$\n",
    "Where $\\tilde{P} = K$, $q = -y$, $G =\\binom{\\text{Diag}(y)}{-\\text{Diag}(y)} $ and $h=\\binom{\\frac{1}{2\\lambda n}\\mathcal{1}}{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "from cvxopt import matrix\n",
    "\n",
    "def solve_dual_SVM(K,y, lambda_ = 1):\n",
    "    n = K.shape[0] \n",
    "    G = np.vstack((np.diag(y),-np.diag(y)))\n",
    "    h = np.vstack(((1/(2*lambda_*n))*np.ones((n,1)),np.zeros((n,1))))\n",
    "\n",
    "    P = K\n",
    "    q = -y.reshape(-1,1)\n",
    "    #P = .5 * (P + P.T)  # make sure P is symmetric\n",
    "    args = [matrix(P), matrix(q)]\n",
    "\n",
    "    args.extend([matrix(G), matrix(h)])\n",
    "\n",
    "    sol = cvxopt.solvers.qp(*args) \n",
    "\n",
    "    return np.array(sol['x']).reshape((P.shape[1],))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_star0 = solve_dual_SVM(K_Xtr0,2*y0-1., lambda_= 0.000001)\n",
    "alpha_star1 = solve_dual_SVM(K_Xtr1,2*y1-1., lambda_= 0.000001)\n",
    "alpha_star2 = solve_dual_SVM(K_Xtr2,2*y2-1., lambda_= 0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Predictions on test set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should parallelize this computation\n",
    "K_Xte0 = GaussKernel(Xtr0_mat100, Xte0_mat100)\n",
    "np.save(\"Kernel_Matrix/gaussian_kernel_Xte0.npy\",K_Xte0)\n",
    "\n",
    "K_Xte1 = GaussKernel(Xtr1_mat100, Xte1_mat100)\n",
    "np.save(\"Kernel_Matrix/gaussian_kernel_Xte1.npy\",K_Xte1)\n",
    "\n",
    "K_Xte2 = GaussKernel(Xtr2_mat100, Xte2_mat100)\n",
    "np.save(\"Kernel_Matrix/gaussian_kernel_Xte2.npy\",K_Xte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction0 = alpha_star0.reshape(-1,1).T.dot(K_Xte0)\n",
    "prediction0[prediction0>0] = 1\n",
    "prediction0[prediction0 <0] = 0\n",
    "\n",
    "prediction1 = alpha_star1.reshape(-1,1).T.dot(K_Xte1)\n",
    "prediction1[prediction1>0] = 1\n",
    "prediction1[prediction1 <0] = 0\n",
    "\n",
    "prediction2 = alpha_star2.reshape(-1,1).T.dot(K_Xte2)\n",
    "prediction2[prediction2>0] = 1\n",
    "prediction2[prediction2 <0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction0 = (np.sign(alpha_star0.reshape(-1,1).T.dot(K_Xtr0))+1)/2\n",
    "print('Train Accuracy 0:',1- np.abs(train_prediction0 - y0).sum()/y0.shape[0])\n",
    "\n",
    "train_prediction1 = (np.sign(alpha_star1.reshape(-1,1).T.dot(K_Xtr1))+1)/2\n",
    "print('Train Accuracy 1:',1- np.abs(train_prediction1 - y1).sum()/y1.shape[0])\n",
    "\n",
    "train_prediction2 = (np.sign(alpha_star2.reshape(-1,1).T.dot(K_Xtr2))+1)/2\n",
    "print('Train Accuracy 2:',1 - np.abs(train_prediction2 - y2).sum()/y2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Writting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.squeeze(np.hstack((prediction0, prediction1, prediction2))).astype(int)\n",
    "df = pd.DataFrame({'Bound': predictions,\n",
    "                   'Id': np.arange(3000)})\n",
    "df = df[['Id','Bound']]\n",
    "\n",
    "df.to_csv(\"Predictions/gaussian_SVM.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Sickit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(Xtr0_mat100, y0)\n",
    "predciton0_sk = clf.predict(Xte0_mat100)\n",
    "\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(Xtr1_mat100, y1)\n",
    "predciton1_sk = clf.predict(Xte1_mat100)\n",
    "\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(Xtr2_mat100, y2)\n",
    "predciton2_sk = clf.predict(Xte2_mat100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spectrum Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Define the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubString(mString, spectrum):\n",
    "    \n",
    "    dictionnary = {}\n",
    "    for i in range(len(mString)-spectrum+1):\n",
    "        if mString[i:i+spectrum] in dictionnary:\n",
    "            dictionnary[mString[i:i+spectrum]] += 1\n",
    "        else:\n",
    "            dictionnary[mString[i:i+spectrum]] = 1\n",
    "    return dictionnary\n",
    "\n",
    "def SpectrumKernelFunction(mString1, mString2, spectrum):\n",
    "    dictionnary1 = getSubString(mString1, spectrum)\n",
    "    dictionnary2 = getSubString(mString2, spectrum)\n",
    "    \n",
    "    kernel = 0\n",
    "    for i in dictionnary1:\n",
    "        if i in dictionnary2:\n",
    "            kernel += dictionnary1[i] * dictionnary2[i]\n",
    "    return kernel\n",
    "\n",
    "## We should improve this function to take less time\n",
    "def SpectrumKernelMatrix_train(serie,spectrum):\n",
    "    n = serie.shape[0]\n",
    "    K = np.zeros((n,n))\n",
    "    for i,seq1 in enumerate(serie):\n",
    "        for j,seq2 in enumerate(serie):\n",
    "            if i <= j :\n",
    "                K[i,j] = SpectrumKernelFunction(seq1, seq2, spectrum)\n",
    "                K[j,i] = K[i,j]\n",
    "    return(K)\n",
    "\n",
    "def SpectrumKernelMatrix_test(serie_train, serie_test, spectrum):\n",
    "    n = serie_train.shape[0]\n",
    "    m = serie_test.shape[0]\n",
    "    K = np.zeros((n,m))\n",
    "    for i,seq1 in enumerate(serie_test):\n",
    "        for j,seq2 in enumerate(serie_train):\n",
    "            K[j,i] = SpectrumKernelFunction(seq1, seq2, spectrum)\n",
    "    return(K)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the Kernel matrix for each of the tree train sets and we save them in *Kernel_Matrix* directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should parallelize this computation\n",
    "\n",
    "if os.path.isfile(\"Kernel_Matrix/spectrum_kernel_Xtr0.npy\"):\n",
    "    K_Xtr0 = np.load(\"Kernel_Matrix/spectrum_kernel_Xtr0.npy\")\n",
    "else:\n",
    "    K_Xtr0 = SpectrumKernelMatrix_train(df_Xtr0['seq'],spectrum=3)\n",
    "    np.save(\"Kernel_Matrix/spectrum_kernel_Xtr0.npy\",K_Xtr0)\n",
    "\n",
    "if os.path.isfile(\"Kernel_Matrix/spectrum_kernel_Xtr1.npy\"):\n",
    "    K_Xtr1 = np.load(\"Kernel_Matrix/spectrum_kernel_Xtr1.npy\")\n",
    "else:\n",
    "    K_Xtr1 = SpectrumKernelMatrix_train(df_Xtr0['seq'],spectrum=3)\n",
    "    np.save(\"Kernel_Matrix/spectrum_kernel_Xtr1.npy\",K_Xtr1)\n",
    "\n",
    "if os.path.isfile(\"Kernel_Matrix/spectrum_kernel_Xtr2.npy\"):\n",
    "    K_Xtr2 = np.load(\"Kernel_Matrix/spectrum_kernel_Xtr2.npy\")\n",
    "else:\n",
    "    K_Xtr2 = SpectrumKernelMatrix_train(df_Xtr0['seq'],spectrum=3)\n",
    "    np.save(\"Kernel_Matrix/spectrum_kernel_Xtr2.npy\",K_Xtr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the Kernel matrix for each of the tree test sets and we save them in *Kernel_Matrix* directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should parallelize this computation\n",
    "if os.path.isfile(\"Kernel_Matrix/spectrum_kernel_Xte0.npy\"):\n",
    "    K_Xte0 = np.load(\"Kernel_Matrix/spectrum_kernel_Xte0.npy\")\n",
    "else:\n",
    "    K_Xte0 = SpectrumKernelMatrix_test(df_Xtr0['seq'],df_Xte0['seq'],spectrum=3)\n",
    "    np.save(\"Kernel_Matrix/spectrum_kernel_Xte0.npy\",K_Xtr0)\n",
    "\n",
    "if os.path.isfile(\"Kernel_Matrix/spectrum_kernel_Xte1.npy\"):\n",
    "    K_Xte1 = np.load(\"Kernel_Matrix/spectrum_kernel_Xte1.npy\")\n",
    "else:\n",
    "    K_Xte1 = SpectrumKernelMatrix_test(df_Xtr1['seq'],df_Xte1['seq'],spectrum=3)\n",
    "    np.save(\"Kernel_Matrix/spectrum_kernel_Xte1.npy\",K_Xtr1)\n",
    "\n",
    "if os.path.isfile(\"Kernel_Matrix/spectrum_kernel_Xte2.npy\"):\n",
    "    K_Xte2 = np.load(\"Kernel_Matrix/spectrum_kernel_Xte2.npy\")\n",
    "else:\n",
    "    K_Xte2 = SpectrumKernelMatrix_test(df_Xtr2['seq'],df_Xte2['seq'],spectrum=3)\n",
    "    np.save(\"Kernel_Matrix/spectrum_kernel_Xte2.npy\",K_Xtr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Solve the standard weighted kernel logisitc regression (WKLR) problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "### We need to improve this ####\n",
    "def sqrtMatrix(W):\n",
    "    # To compute the square root of a symetric positive matrix\n",
    "    D,V = np.linalg.eig(W)\n",
    "    return np.dot(np.dot(V,np.diag(np.sqrt(D))),np.linalg.inv(V))\n",
    "\n",
    "def solveWKRR(K,W,z,lambda_):\n",
    "    n = K.shape[0]\n",
    "    W_sqrt = np.real(sqrtMatrix(W))\n",
    "    \n",
    "    temp = np.dot(np.dot(W_sqrt,K),W_sqrt) +  n*lambda_*np.eye(n)\n",
    "    return  np.dot(W_sqrt,np.linalg.solve(temp,np.dot(W_sqrt,z)))\n",
    "\n",
    "def solveKLR(K,y,alpha0,lambda_ = 1,itermax = 30, eps =1e-6):\n",
    "    n = K.shape[0]\n",
    "    \n",
    "    iter_ = 0\n",
    "    last_alpha = 10*alpha0 + np.ones(alpha0.shape)\n",
    "    alpha = alpha0\n",
    "    \n",
    "    while (iter_< itermax) and (np.linalg.norm(last_alpha-alpha)>eps) :         \n",
    "        print(iter_,np.linalg.norm(last_alpha-alpha))\n",
    "        last_alpha = alpha\n",
    "        m = np.dot(K,alpha)\n",
    "        P = np.zeros((n,1))\n",
    "        W = np.zeros((n,n))\n",
    "        z = np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            P[i,0] = -sigmoid(-y[i]*m[i])\n",
    "            W[i,i] = sigmoid(m[i])*sigmoid(-m[i])\n",
    "            z[i,0] = m[i] - (P[i,0]*y[i])/W[i,i]\n",
    "        alpha = solveWKRR(K,W,z,lambda_)\n",
    "        iter_ = iter_ +1\n",
    "        \n",
    "      \n",
    "    return alpha        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K0 = K_Xtr0\n",
    "y_0 = y0.reshape((y0.shape[0],1))\n",
    "y_0 = 2*y_0-1\n",
    "n = y_0.shape[0]\n",
    "alpha0 = np.zeros((n,1))\n",
    "alpha_0 = solveKLR(K0,y_0,alpha0,10) \n",
    "\n",
    "K1 = K_Xtr1\n",
    "y_1 = y1.reshape((y1.shape[0],1))\n",
    "y_1 = 2*y_1-1\n",
    "n = y_1.shape[0]\n",
    "alpha0 = np.zeros((n,1))\n",
    "alpha_1 = solveKLR(K1,y_1,alpha0,10) \n",
    "\n",
    "K2 = K_Xtr2\n",
    "y_2 = y2.reshape((y2.shape[0],1))\n",
    "y_2 = 2*y_2-1\n",
    "n = y_2.shape[0]\n",
    "alpha0 = np.zeros((n,1))\n",
    "alpha_2 = solveKLR(K2,y_2,alpha0,10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    y = x\n",
    "    n = x.shape[0]\n",
    "    for i in range(n):\n",
    "        if x[i,0] > 0:\n",
    "            y[i,0] = 1\n",
    "        else:\n",
    "            y[i,0] = 0\n",
    "    return y\n",
    "\n",
    "print('Accuracy:',np.linalg.norm(1-sign(np.dot(K0,alpha_0))+y_0,1)/y_0.shape[0])\n",
    "print('Accuracy:',np.linalg.norm(1-sign(np.dot(K1,alpha_1))+y_1,1)/y_0.shape[0])\n",
    "print('Accuracy:',np.linalg.norm(1-sign(np.dot(K2,alpha_2))+y_2,1)/y_0.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mismatch Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from itertools import product\n",
    "\n",
    "ngrams = lambda a, n: list(zip(*[a[i:] for i in range(n)])) #function that extract all the n grams in a given sequence\n",
    "\n",
    "def AllPossibleCombinationlist(char_list,n):\n",
    "    '''\n",
    "    Compute all the possible ngrams that we can obtain from a list of char \n",
    "    This function will allow us to have a correspondance between all our histograms representing\n",
    "    each sequences because the bin i will represent the same n gram (given by the i-th value of the list\n",
    "    that we are returning) in all our histograms\n",
    "    Param: char_list: (list) list of possible char\n",
    "    n: (int) n in ngram - length of the subsequences considered\n",
    "    '''\n",
    "    #n corresponds to n in n gram\n",
    "    return list(product(char_list,repeat=n))\n",
    "\n",
    "def CreateHistogramSeq(Seq,AllCombinList,n):\n",
    "    '''\n",
    "    Create the embedding that allows to compute the spectrum kernel: histogram of all the subsequences of length n\n",
    "    in the sequence\n",
    "    Param: Seq: (str) DNA sequence containing only the letter A,C,G,T\n",
    "    n: (int) length of the subsequences considered\n",
    "    AllCombinList: (list) a list containing all the possible combination of length n that we can compute using the letters\n",
    "    A C G T\n",
    "    Return: value : np.array contains the representation of the sequence as an array\n",
    "    '''\n",
    "    decompose_seq= ngrams(Seq,n)\n",
    "    value = np.zeros([len(AllCombinList),])\n",
    "    for ngram in decompose_seq:\n",
    "        index_ngram = AllCombinList.index(ngram)\n",
    "        value[index_ngram] = value[index_ngram]+1\n",
    "    return value\n",
    "\n",
    "def CreateHistogramMismatchSeq(Seq,AllCombinList,n):\n",
    "    '''\n",
    "    Create the embedding that allows to compute the mismatch kernel: histogram of all the subsequences of length n\n",
    "    in the sequence. This time allows one mismatch.\n",
    "    Param: @Seq: (str) DNA sequence containing only the letter A,C,G,T\n",
    "    @n: (int) length of the subsequences considered\n",
    "    @AllCombinList: (list) a list containing all the possible combination of length n that we can compute using the letters\n",
    "    A C G T\n",
    "    Return: value : np.array contains the representation of the sequence as an array\n",
    "    '''\n",
    "    letters = ['A','C','G','T']\n",
    "    decompose_seq= ngrams(Seq,n)\n",
    "    value = np.zeros([len(AllCombinList),])\n",
    "    for ngram in decompose_seq:\n",
    "        index_ngram = AllCombinList.index(ngram)\n",
    "        value[index_ngram] = value[index_ngram]+1\n",
    "        copy_ngram = list(ngram)\n",
    "        for ind,cur_letter in enumerate(copy_ngram):\n",
    "            for letter in letters:\n",
    "                if letter!=cur_letter:\n",
    "                    new_ngram = list(copy_ngram)\n",
    "                    new_ngram[ind]= letter\n",
    "                    mismatch_ngram = tuple(new_ngram)\n",
    "                    index_ngram = AllCombinList.index(mismatch_ngram)\n",
    "                    value[index_ngram] = value[index_ngram]+0.1\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "def compute_idf(list_histograms):\n",
    "    '''\n",
    "    Compute the idf score for all the subsequences that appears in our computation.\n",
    "    If a sequences appears rarely it will have a higher score than if it appears really frequently\n",
    "    Param: @list_histograms: list of histograms that as been computed\n",
    "    Return: (np.array) compute the idf score for all the bins in the histogram\n",
    "    '''\n",
    "    idf = 0.000001*np.ones((list_histograms.shape[1]))\n",
    "    for sent in list_histograms:\n",
    "        idf += np.array(sent)\n",
    "    \n",
    "    idf= np.maximum(1, np.log10(len(list_histograms) / (idf)))\n",
    "    \n",
    "    return idf\n",
    "\n",
    "\n",
    "def compute_kernel_histogram(x1,x2):\n",
    "    \"\"\" \n",
    "    Compute the scalar product between x1 and x2 (linear kernel in the embedding given in the Spectrum Kernel space)\n",
    "    Param: @x1: (np.array) data 1 to use to feed the linear kernel computation\n",
    "    @x2: (np.array) data 2 to use to feed the linear kernel computation\n",
    "    \"\"\"\n",
    "    value= np.vdot(x1,x2)\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "In order to allow the use of parallelization and the \n",
    "multiprocessing library we have computed some really basic functions using classes\n",
    "The next few functions must be easy to understand\n",
    "'''\n",
    "compute_diag = lambda X,i: compute_kernel_histogram(X[i], X[i])\n",
    "compute_element_kernel_square = lambda X1,sim_docs_kernel_value,i,j: compute_kernel_histogram(X1[i], X1[j])/(sim_docs_kernel_value[i] *sim_docs_kernel_value[j])**0.5\n",
    "compute_element_kernel = lambda X1,X2,sim_docs_kernel_value,i,j: compute_kernel_histogram(X1[i], X2[j])/(sim_docs_kernel_value[1][i] *sim_docs_kernel_value[2][j])**0.5\n",
    "\n",
    "\n",
    "class compute_diag_copy(object):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "    def __call__(self, i):\n",
    "        return compute_diag(self.X,i)\n",
    "\n",
    "class compute_element_i(object):\n",
    "    def __init__(self, X,sim_docs_kernel_value,i):\n",
    "        self.X = X\n",
    "        self.sim_docs_kernel_value = sim_docs_kernel_value\n",
    "        self.i = i\n",
    "    def __call__(self, j):\n",
    "        return compute_element_kernel_square(self.X,self.sim_docs_kernel_value,self.i,j)\n",
    "\n",
    "class compute_element_i_general(object):\n",
    "    def __init__(self, X,X_p,sim_docs_kernel_value,i):\n",
    "        self.X = X\n",
    "        self.X_p = X_p\n",
    "        self.sim_docs_kernel_value = sim_docs_kernel_value\n",
    "        self.i = i\n",
    "    def __call__(self, j):\n",
    "        return compute_element_kernel(self.X,self.X_p,self.sim_docs_kernel_value,self.i,j)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def histogram_kernel(X1,X2=[]):\n",
    "    '''\n",
    "    This function computes the spectrum kernel gram matrix. (Because we assume that X1 and X2 are given\n",
    "    in the rkhs space this kernel is equivalent to a basic linear kernel)\n",
    "    Param: @X1: (np.array)(nb_sample,nb_features) Training data\n",
    "    @X2: (np.array)(nb_sample,nb_features) Testing data (if empty compute the gram matrix for training else compute\n",
    "    the gram matrix for testing)\n",
    "    @n_proc: (int) allows to use more processor in order to compute the gram matrix quickly\n",
    "    Return: Spectrum Kernel Gram matrix\n",
    "    '''\n",
    "\n",
    "    len_X2= len(X2)\n",
    "    len_X1 = len(X1)\n",
    "    sim_docs_kernel_value = {}\n",
    "    if len_X2 ==0:\n",
    "        gram_matrix = np.zeros((len_X1, len_X1), dtype=np.float32)\n",
    "        for i in range(len_X1):\n",
    "            sim_docs_kernel_value[i] = compute_diag_copy(X1)(i)\n",
    "                            \n",
    "        for i in range(len_X1):\n",
    "            for j in range(i,len_X1):    \n",
    "                gram_matrix[i, j]= compute_element_i(X1,sim_docs_kernel_value,i)(j)\n",
    "                gram_matrix[j, i] = gram_matrix[i, j]\n",
    "        #calculate Gram matrix\n",
    "        return gram_matrix\n",
    "    \n",
    "    else:\n",
    "        gram_matrix = np.zeros((len_X1, len_X2), dtype=np.float32)\n",
    "    \n",
    "        sim_docs_kernel_value[1] = {}\n",
    "        sim_docs_kernel_value[2] = {}\n",
    "        for i in range(len_X1):\n",
    "            sim_docs_kernel_value[1][i] = compute_diag_copy(X1)(i)\n",
    "        for j in range(len_X2):\n",
    "            sim_docs_kernel_value[2][j] = compute_diag_copy(X2)(j)\n",
    "    \n",
    "        for i in range(len_X1):\n",
    "            for j in range(len_X2):\n",
    "                gram_matrix[i, j] = compute_element_i_general(X1,X2,sim_docs_kernel_value,i)(j)    \n",
    "        return gram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranforming into numpy.arrays -- train\n",
    "Xtr0 = list(df_Xtr0['seq'])\n",
    "Xtr1 = list(df_Xtr1['seq'])\n",
    "Xtr2 = list(df_Xtr2['seq'])\n",
    "\n",
    "# Tranforming into numpy.arrays -- test\n",
    "Xte0 = list(df_Xte0['seq'])\n",
    "Xte1 = list(df_Xte1['seq'])\n",
    "Xte2 = list(df_Xte2['seq'])\n",
    "\n",
    "# Transforming the labels into numpy.arrays \n",
    "y0 = np.array(df_Ytr0)[:,1]\n",
    "y1 = np.array(df_Ytr1)[:,1]\n",
    "y2 = np.array(df_Ytr2)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nngram = 7 #param\n",
    "list_all_combin_DNA = AllPossibleCombinationlist(['A','C','G','T'],nngram)\n",
    "X_train_histo_0 = np.empty([len(Xtr0),len(list_all_combin_DNA)])\n",
    "X_test_histo_0 = np.empty([len(Xte0),len(list_all_combin_DNA)])\n",
    "X_train_histo_1 = np.empty([len(Xtr1),len(list_all_combin_DNA)])\n",
    "X_test_histo_1 = np.empty([len(Xte1),len(list_all_combin_DNA)])\n",
    "X_train_histo_2 = np.empty([len(Xtr2),len(list_all_combin_DNA)])\n",
    "X_test_histo_2 = np.empty([len(Xte2),len(list_all_combin_DNA)])\n",
    "\n",
    "for i in range(len(Xtr0)):\n",
    "    X_train_histo_0[i,:] = CreateHistogramMismatchSeq(Xtr0[i],list_all_combin_DNA,nngram)\n",
    "    \n",
    "for j in range(len(Xte0)):\n",
    "    X_test_histo_0[j,:] = CreateHistogramMismatchSeq(Xte0[j],list_all_combin_DNA,nngram)\n",
    "\n",
    "    \n",
    "for i in range(len(Xtr1)):\n",
    "    X_train_histo_1[i,:] = CreateHistogramMismatchSeq(Xtr1[i],list_all_combin_DNA,nngram)\n",
    "    \n",
    "for j in range(len(Xte1)):\n",
    "    X_test_histo_1[j,:] = CreateHistogramMismatchSeq(Xte1[j],list_all_combin_DNA,nngram)\n",
    "\n",
    "\n",
    "for i in range(len(Xtr2)):\n",
    "    X_train_histo_2[i,:] = CreateHistogramMismatchSeq(Xtr2[i],list_all_combin_DNA,nngram)\n",
    "    \n",
    "for j in range(len(Xte2)):\n",
    "    X_test_histo_2[j,:] = CreateHistogramMismatchSeq(Xte2[j],list_all_combin_DNA,nngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split_0 = X_train_histo_0\n",
    "y_train_split_0 = y0\n",
    "y_train_split_0[y_train_split_0==0]=-1\n",
    "\n",
    "X_train_split_1 = X_train_histo_1\n",
    "y_train_split_1 = y1\n",
    "y_train_split_1[y_train_split_1==0]=-1 \n",
    "\n",
    "X_train_split_2 = X_train_histo_2\n",
    "y_train_split_2 = y2\n",
    "y_train_split_2[y_train_split_2==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################Compute the train gram matrices#######################\n",
    "gram_train_multi_proc = histogram_kernel(X_train_split_0)          \n",
    "gram_train_multi_proc_1 = histogram_kernel(X_train_split_1)\n",
    "gram_train_multi_proc_2 = histogram_kernel(X_train_split_2)\n",
    "\n",
    "#################Compute test gram matrices#######################\n",
    "gram_test_final_0 =  histogram_kernel(X_train_split_0,X_test_histo_0)\n",
    "gram_test_final_1 =  histogram_kernel(X_train_split_1,X_test_histo_1)\n",
    "gram_test_final_2 =  histogram_kernel(X_train_split_2,X_test_histo_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_train_multi_proc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Define Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strkernel.mismatch_kernel import MismatchKernel, preprocess\n",
    "\n",
    "after_process = preprocess(list(df_Xtr0['seq']) + list(df_Xte0['seq']))\n",
    "mismatch_kernel = MismatchKernel(l=4, k=5, m=1).get_kernel(after_process)\n",
    "K_Xtr0 = mismatch_kernel.kernel\n",
    "np.save(\"Kernel_Matrix/mismatch_kernel_Xtr0.npy\",K_Xtr0)\n",
    "\n",
    "after_process = preprocess(list(df_Xtr1['seq']))\n",
    "mismatch_kernel = MismatchKernel(l=4, k=5, m=1).get_kernel(after_process)\n",
    "K_Xtr1 = mismatch_kernel.kernel\n",
    "np.save(\"Kernel_Matrix/mismatch_kernel_Xtr1.npy\",K_Xtr1)\n",
    "\n",
    "after_process = preprocess(list(df_Xtr2['seq']))\n",
    "mismatch_kernel = MismatchKernel(l=4, k=5, m=1).get_kernel(after_process)\n",
    "K_Xtr2 = mismatch_kernel.kernel\n",
    "np.save(\"Kernel_Matrix/mismatch_kernel_Xtr2.npy\",K_Xtr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_process = preprocess(list(df_Xtr0['seq']) + list(df_Xte0['seq']))\n",
    "mismatch_kernel = MismatchKernel(l=4, k=4, m=1).get_kernel(after_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_kernel = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.spectrum_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def k_mers(Alphabet, k):\n",
    "    'Compute all possible words of size k from Alphabet'\n",
    "    'Store the result as a dictionnary where the keys are the words and the values ar integer Ids'\n",
    "    all_kmers_tuple = list(itertools.product(Alphabet, repeat=k))\n",
    "    all_kmers = list(map(lambda tup: ''.join(tup), all_kmers_tuple))\n",
    "    return dict(zip(all_kmers, range(len(all_kmers))))\n",
    "\n",
    "    \n",
    "def spectrum_embedding(sequence, all_kmers_dict, k):\n",
    "    'Compute the k-sepctrum embedding of sequence'\n",
    "    'The result is a vector of size vocabulary'\n",
    "    embedding = np.zeros(len(all_kmers_dict))\n",
    "    for idx in range(len(sequence)-k+1): # slidding window of size k on the sequence\n",
    "        word_id = all_kmers_dict[sequence[idx:idx+k]]\n",
    "        embedding[word_id] += 1  \n",
    "    return(embedding)\n",
    "\n",
    "def data_embedding(df, all_kmers_dict, k):\n",
    "    nb_sequences = df.shape[0]\n",
    "    embedding_dict = dict.fromkeys(range(nb_sequences))\n",
    "    for idx,sequence in enumerate(list(df['seq'])):\n",
    "        embedding = spectrum_embedding(sequence, all_kmers_dict, k)\n",
    "        embedding_dict[idx] = embedding  \n",
    "    return embedding_dict\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the embeddings for all data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "Alphabet = ['G', 'C', 'A', 'T']\n",
    "\n",
    "all_kmers_dict = k_mers(Alphabet, k)\n",
    "\n",
    "\n",
    "Xtr0_embedding = data_embedding(df_Xtr0,all_kmers_dict,k)\n",
    "Xtr1_embedding = data_embedding(df_Xtr1,all_kmers_dict,k)\n",
    "Xtr2_embedding = data_embedding(df_Xtr2,all_kmers_dict,k)\n",
    "\n",
    "Xte0_embedding = data_embedding(df_Xte0,all_kmers_dict,k)\n",
    "Xte1_embedding = data_embedding(df_Xte1,all_kmers_dict,k)\n",
    "Xte2_embedding = data_embedding(df_Xte2,all_kmers_dict,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Computing tf_idf scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting_array(embedding_dict, all_kmers_dict):\n",
    "    'Compute the count matrix of kmer by sequence'\n",
    "    'Output is an array of size nb_seq*vocab_size'\n",
    "    D = len(embedding_dict) # Number of documents\n",
    "    T = len(all_kmers_dict) # Vocabulary size\n",
    "    \n",
    "    output = np.zeros((D,T))\n",
    "    for document in embedding_dict.keys():\n",
    "        output[document] = embedding_dict[document]\n",
    "    return output\n",
    "\n",
    "def tf_idf(D):\n",
    "    'Input : D is a conting matrix (nb_seq*vocab_size)'\n",
    "    'Ouptut: array of tf_idf socres'\n",
    "    N = D.shape[0] # number of documents\n",
    "    idf = np.log(N/np.count_nonzero(D,axis=1)).reshape(-1,1)\n",
    "    tf = np.log(1+D/np.sum(D,axis = 1).reshape(-1,1))\n",
    "    return tf*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the tf_idf for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf-Idf numpy.arrays -- train\n",
    "D_tr0 = counting_array(Xtr0_embedding, all_kmers_dict)\n",
    "Xtr0 = tf_idf(D_tr0)\n",
    "\n",
    "D_tr1 = counting_array(Xtr1_embedding, all_kmers_dict)\n",
    "Xtr1 = tf_idf(D_tr1)\n",
    "\n",
    "D_tr2 = counting_array(Xtr2_embedding, all_kmers_dict)\n",
    "Xtr2 = tf_idf(D_tr2)\n",
    "\n",
    "\n",
    "# Tf-Idf numpy.arrays -- test\n",
    "D_te0 = counting_array(Xte0_embedding, all_kmers_dict)\n",
    "Xte0 = tf_idf(D_te0)\n",
    "\n",
    "D_te1 = counting_array(Xte1_embedding, all_kmers_dict)\n",
    "Xte1 = tf_idf(D_te1)\n",
    "\n",
    "D_te2 = counting_array(Xte2_embedding, all_kmers_dict)\n",
    "Xte2 = tf_idf(D_te2)\n",
    "\n",
    "# Transforming the labels into numpy.arrays \n",
    "y0 = 2*np.array(df_Ytr0)[:,1] - 1\n",
    "y1 = 2*np.array(df_Ytr1)[:,1] - 1\n",
    "y2 = 2*np.array(df_Ytr2)[:,1] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.  SVM + Guaussian on tf_idf embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute guassian kernel based on this embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should parallelize this computation\n",
    "if os.path.isfile(\"Kernel_Matrix/tf_idf_kernel_Xtr0.npy\"):\n",
    "    K_Xtr0 = np.load(\"Kernel_Matrix/tf_idf_kernel_Xtr0.npy\")\n",
    "else:\n",
    "    K_Xtr0 = GaussKernel(Xtr0, Xtr0, sigma = 0.1)\n",
    "    np.save(\"Kernel_Matrix/tf_idf_kernel_Xtr0.npy\",K_Xtr0)\n",
    "    \n",
    "if os.path.isfile(\"Kernel_Matrix/tf_idf_kernel_Xtr1.npy\"):\n",
    "    K_Xtr1 = np.load(\"Kernel_Matrix/tf_idf_kernel_Xtr1.npy\")\n",
    "else:\n",
    "    K_Xtr1 = GaussKernel(Xtr1, Xtr1, sigma = 0.1)\n",
    "    np.save(\"Kernel_Matrix/tf_idf_kernel_Xtr1.npy\",K_Xtr1)\n",
    "    \n",
    "if os.path.isfile(\"Kernel_Matrix/tf_idf_kernel_Xtr2.npy\"):\n",
    "    K_Xtr2 = np.load(\"Kernel_Matrix/tf_idf_kernel_Xtr2.npy\")\n",
    "else:\n",
    "    K_Xtr2 = GaussKernel(Xtr2, Xtr2, sigma = 0.1)\n",
    "    np.save(\"Kernel_Matrix/tf_idf_kernel_Xtr1.npy\",K_Xtr2)\n",
    "    \n",
    "if os.path.isfile(\"Kernel_Matrix/tf_idf_kernel_Xte0.npy\"):\n",
    "    K_Xte0 = np.load(\"Kernel_Matrix/tf_idf_kernel_Xte0.npy\")\n",
    "else:\n",
    "    K_Xte0 = GaussKernel(Xtr0, Xte0, sigma = 0.1)\n",
    "    np.save(\"Kernel_Matrix/tf_idf_kernel_Xte1.npy\",K_Xte0)\n",
    "\n",
    "if os.path.isfile(\"Kernel_Matrix/tf_idf_kernel_Xte1.npy\"):\n",
    "    K_Xte1 = np.load(\"Kernel_Matrix/tf_idf_kernel_Xte1.npy\")\n",
    "else:\n",
    "    K_Xte1 = GaussKernel(Xtr1, Xte1, sigma = 0.1)\n",
    "    np.save(\"Kernel_Matrix/tf_idf_kernel_Xte1.npy\",K_Xte1)\n",
    "    \n",
    "if os.path.isfile(\"Kernel_Matrix/tf_idf_kernel_Xte2.npy\"):\n",
    "    K_Xte2 = np.load(\"Kernel_Matrix/tf_idf_kernel_Xte2.npy\")\n",
    "else:\n",
    "    K_Xte2 = GaussKernel(Xtr2, Xte2, sigma = 0.1)\n",
    "    np.save(\"Kernel_Matrix/tf_idf_kernel_Xte1.npy\",K_Xte2)\n",
    " \n",
    "\n",
    "# Transforming the labels into numpy.arrays \n",
    "y0 = 2*np.array(df_Ytr0)[:,1] - 1.\n",
    "y1 = 2*np.array(df_Ytr1)[:,1] - 1.\n",
    "y2 = 2*np.array(df_Ytr2)[:,1] - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve SVM like in part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  6.9775e+10 -2.1049e+11  3e+11  5e-17  3e-11\n",
      " 1:  1.6532e+10 -2.4954e+10  4e+10  2e-16  2e-11\n",
      " 2:  2.4917e+09 -2.9045e+09  5e+09  2e-16  9e-12\n",
      " 3:  3.5990e+08 -4.0697e+08  8e+08  2e-16  3e-12\n",
      " 4:  5.1575e+07 -5.7631e+07  1e+08  2e-16  1e-12\n",
      " 5:  7.3441e+06 -8.2776e+06  2e+07  2e-16  5e-13\n",
      " 6:  1.0322e+06 -1.2046e+06  2e+06  2e-16  2e-13\n",
      " 7:  1.3983e+05 -1.8047e+05  3e+05  2e-16  8e-14\n",
      " 8:  1.6649e+04 -2.9087e+04  5e+04  2e-16  2e-14\n",
      " 9:  7.6100e+02 -5.6383e+03  6e+03  2e-16  1e-14\n",
      "10: -8.9775e+02 -1.6757e+03  8e+02  2e-16  3e-15\n",
      "11: -9.8192e+02 -1.0202e+03  4e+01  2e-16  2e-15\n",
      "12: -9.8227e+02 -9.8293e+02  7e-01  2e-16  1e-15\n",
      "13: -9.8227e+02 -9.8228e+02  7e-03  2e-16  1e-15\n",
      "14: -9.8227e+02 -9.8227e+02  7e-05  2e-16  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  7.1272e+10 -2.2178e+11  3e+11  5e-17  3e-11\n",
      " 1:  1.5578e+10 -1.7887e+10  3e+10  2e-16  2e-11\n",
      " 2:  2.2706e+09 -2.6716e+09  5e+09  2e-16  9e-12\n",
      " 3:  3.2711e+08 -3.6473e+08  7e+08  2e-16  3e-12\n",
      " 4:  4.6840e+07 -5.2217e+07  1e+08  2e-16  1e-12\n",
      " 5:  6.6682e+06 -7.5065e+06  1e+07  2e-16  6e-13\n",
      " 6:  9.3715e+05 -1.0931e+06  2e+06  2e-16  2e-13\n",
      " 7:  1.2697e+05 -1.6377e+05  3e+05  2e-16  7e-14\n",
      " 8:  1.5126e+04 -2.6391e+04  4e+04  2e-16  2e-14\n",
      " 9:  6.9408e+02 -5.1146e+03  6e+03  2e-16  9e-15\n",
      "10: -8.1564e+02 -1.5219e+03  7e+02  2e-16  3e-15\n",
      "11: -8.9363e+02 -9.2923e+02  4e+01  2e-16  2e-15\n",
      "12: -8.9404e+02 -8.9491e+02  9e-01  2e-16  1e-15\n",
      "13: -8.9404e+02 -8.9405e+02  9e-03  2e-16  1e-15\n",
      "14: -8.9404e+02 -8.9404e+02  9e-05  2e-16  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  7.0318e+10 -2.0889e+11  3e+11  5e-17  3e-11\n",
      " 1:  1.5510e+10 -1.9820e+10  4e+10  2e-16  2e-11\n",
      " 2:  2.2854e+09 -2.6639e+09  5e+09  2e-16  8e-12\n",
      " 3:  3.2947e+08 -3.6933e+08  7e+08  2e-16  2e-12\n",
      " 4:  4.7185e+07 -5.2648e+07  1e+08  2e-16  1e-12\n",
      " 5:  6.7162e+06 -7.5690e+06  1e+07  2e-16  5e-13\n",
      " 6:  9.4329e+05 -1.1026e+06  2e+06  2e-16  1e-13\n",
      " 7:  1.2753e+05 -1.6542e+05  3e+05  2e-16  7e-14\n",
      " 8:  1.5068e+04 -2.6757e+04  4e+04  2e-16  2e-14\n",
      " 9:  6.1083e+02 -5.2325e+03  6e+03  2e-16  9e-15\n",
      "10: -8.8184e+02 -1.5850e+03  7e+02  2e-16  3e-15\n",
      "11: -9.5496e+02 -9.8775e+02  3e+01  2e-16  1e-15\n",
      "12: -9.5526e+02 -9.5692e+02  2e+00  2e-16  1e-15\n",
      "13: -9.5526e+02 -9.5529e+02  3e-02  2e-16  1e-15\n",
      "14: -9.5526e+02 -9.5526e+02  3e-04  2e-16  1e-15\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "alpha_star0 = solve_dual_SVM(K_Xtr0,y0, lambda_= 1e-8)\n",
    "alpha_star1 = solve_dual_SVM(K_Xtr1,y1, lambda_= 1e-8)\n",
    "alpha_star2 = solve_dual_SVM(K_Xtr2,y2, lambda_= 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction0 = alpha_star0.reshape(-1,1).T.dot(K_Xte0)\n",
    "prediction0[prediction0>0] = 1\n",
    "prediction0[prediction0 <0] = 0\n",
    "\n",
    "prediction1 = alpha_star1.reshape(-1,1).T.dot(K_Xte1)\n",
    "prediction1[prediction1>0] = 1\n",
    "prediction1[prediction1 <0] = 0\n",
    "\n",
    "prediction2 = alpha_star2.reshape(-1,1).T.dot(K_Xte2)\n",
    "prediction2[prediction2>0] = 1\n",
    "prediction2[prediction2 <0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0: 0.48850000000000005\n",
      "Train Accuracy 1: 0.499\n",
      "Train Accuracy 2: 0.49950000000000006\n"
     ]
    }
   ],
   "source": [
    "train_prediction0 = (np.sign(alpha_star0.reshape(-1,1).T.dot(K_Xtr0))+1)/2\n",
    "print('Train Accuracy 0:',1- np.abs(train_prediction0 - y0).sum()/y0.shape[0])\n",
    "\n",
    "train_prediction1 = (np.sign(alpha_star1.reshape(-1,1).T.dot(K_Xtr1))+1)/2\n",
    "print('Train Accuracy 1:',1- np.abs(train_prediction1 - y1).sum()/y1.shape[0])\n",
    "\n",
    "train_prediction2 = (np.sign(alpha_star2.reshape(-1,1).T.dot(K_Xtr2))+1)/2\n",
    "print('Train Accuracy 2:',1 - np.abs(train_prediction2 - y2).sum()/y2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Writting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.squeeze(np.hstack((prediction0, prediction1, prediction2))).astype(int)\n",
    "df = pd.DataFrame({'Bound': predictions,\n",
    "                   'Id': np.arange(3000)})\n",
    "df = df[['Id','Bound']]\n",
    "\n",
    "df.to_csv(\"Predictions/tf_idf_SVM.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
